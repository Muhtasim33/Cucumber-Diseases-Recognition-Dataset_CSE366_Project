{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13393812,"sourceType":"datasetVersion","datasetId":8499236}],"dockerImageVersionId":31192,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Import Libraries**","metadata":{}},{"cell_type":"code","source":"import os\nimport random\nimport math\nfrom glob import glob\nfrom collections import Counter\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom PIL import Image\nimport cv2\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras import layers, models\n\n\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-18T02:27:48.535338Z","iopub.execute_input":"2025-10-18T02:27:48.535713Z","iopub.status.idle":"2025-10-18T02:27:57.972733Z","shell.execute_reply.started":"2025-10-18T02:27:48.535688Z","shell.execute_reply":"2025-10-18T02:27:57.971861Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 1. **Analyze and Report on class balance to identify potential biases.**","metadata":{}},{"cell_type":"code","source":"\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nDATA_ROOT = \"/kaggle/input/cucumber-disease-recognition-dataset/Original Image\"\nTRAIN_DIR = os.path.join(DATA_ROOT, \"train\") if os.path.exists(os.path.join(DATA_ROOT, \"train\")) else DATA_ROOT\n\n#finding class subfolders\nclasses = [d for d in os.listdir(TRAIN_DIR) if os.path.isdir(os.path.join(TRAIN_DIR, d))]  \nclasses.sort()\n\nclass_counts = {}\nfor cls in classes:\n    path = os.path.join(TRAIN_DIR, cls)\n    files = [f for f in os.listdir(path) if os.path.splitext(f)[1].lower() in ('.jpg', '.jpeg', '.png')]\n    class_counts[cls] = len(files)\n\n# Arranging data in column or tables\ndf_counts = pd.DataFrame.from_dict(class_counts, orient='index', columns=['count']).sort_values('count', ascending=False)\ndisplay(df_counts)\n\n# visualize class balance\nplt.figure(figsize=(10, 4))\ndf_counts_reset = df_counts.reset_index().rename(columns={'index': 'class'})\nsns.barplot(x='class', y='count', data=df_counts_reset)\nplt.xticks(rotation=45)\nplt.title(\"Class Balance â€“ Number of Images per Class\")\nplt.xlabel(\"Class\")\nplt.ylabel(\"Image Count\")\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T02:27:57.974054Z","iopub.execute_input":"2025-10-18T02:27:57.974711Z","iopub.status.idle":"2025-10-18T02:27:58.348047Z","shell.execute_reply.started":"2025-10-18T02:27:57.974687Z","shell.execute_reply":"2025-10-18T02:27:58.347056Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **2.Visualize sample images to understand intra-class variation and data quality.**","metadata":{}},{"cell_type":"code","source":"import random\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\nncols = 4\nnrows = len(classes)\nfig, axes = plt.subplots(nrows, ncols, figsize=(12, 3*nrows))\n\nfor i, cls in enumerate(classes):\n    cls_path = os.path.join(TRAIN_DIR, cls)\n    files = [f for f in os.listdir(cls_path) if os.path.splitext(f)[1].lower() in ('.jpg', '.jpeg', '.png')]\n    sample = random.sample(files, min(ncols, len(files)))\n    for j in range(ncols):\n        ax = axes[i, j] if nrows > 1 else axes[j]\n        ax.axis('off')\n        if j < len(sample):\n            img = Image.open(os.path.join(cls_path, sample[j])).convert('RGB')\n            ax.imshow(img)\n        if j == 0:\n            ax.set_title(f\"{cls}\\n({class_counts[cls]} images)\", fontsize=10)\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T02:28:03.765232Z","iopub.execute_input":"2025-10-18T02:28:03.765884Z","iopub.status.idle":"2025-10-18T02:28:20.934924Z","shell.execute_reply.started":"2025-10-18T02:28:03.765855Z","shell.execute_reply":"2025-10-18T02:28:20.933423Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n# **3**. **Investigate image properties such as resolution, aspect ratio, and color distributions to inform preprocessing strategies.**\n","metadata":{}},{"cell_type":"code","source":"import cv2\nimport numpy as np\nfrom collections import Counter\nfrom PIL import Image\n\ndef im_info(path):\n    try:\n        with Image.open(path) as im:\n            w, h = im.size\n            mode = im.mode\n        return w, h, mode\n    except:\n        return None\n\n# get all image paths\nimage_paths = []\nfor cls in classes:\n    folder = os.path.join(TRAIN_DIR, cls)\n    for f in os.listdir(folder):\n        if os.path.splitext(f)[1].lower() in ('.jpg', '.jpeg', '.png'):\n            image_paths.append(os.path.join(folder, f))\n\nprint(f\"Total images scanned: {len(image_paths)}\")\n\nres_list, aspect_list = [], []\nmodes = Counter()\n\nfor p in image_paths:\n    info = im_info(p)\n    if info is None:\n        continue\n    w, h, mode = info\n    res_list.append((w, h))\n    aspect_list.append(w / h if h > 0 else 0)\n    modes[mode] += 1\n\nimport pandas as pd\nres_df = pd.DataFrame(res_list, columns=['width', 'height'])\nres_df['area'] = res_df['width'] * res_df['height']\nres_df['aspect_ratio'] = res_df['width'] / res_df['height']\n\nprint(\"Image color modes:\", dict(modes))\ndisplay(res_df.describe())\n\n# plot resolution and aspect ratio\nplt.figure(figsize=(10,4))\nplt.hist(res_df['area'], bins=40)\nplt.title('Image Area Distribution (pixels)')\nplt.xlabel('Area (w*h)')\nplt.ylabel('Count')\nplt.show()\n\nplt.figure(figsize=(10,4))\nplt.hist(res_df['aspect_ratio'].clip(0.2, 5), bins=40)\nplt.title('Aspect Ratio (w/h) Distribution')\nplt.xlabel('Aspect Ratio')\nplt.ylabel('Count')\nplt.show()\n\n# Color channel distribution (sample) computing only per image means\nsample_paths = random.sample(image_paths, min(50, len(image_paths)))\nr_means, g_means, b_means = [], [], []\n\nfor i, p in enumerate(sample_paths):\n    im = cv2.imread(p)\n    if im is None:\n        continue\n    im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n    mean_vals = im.mean(axis=(0,1))  # mean for R,G,B\n    r_means.append(mean_vals[0])\n    g_means.append(mean_vals[1])\n    b_means.append(mean_vals[2])\n    if i % 10 == 0:\n        print(f\"Processed {i+1}/{len(sample_paths)} images\")\n\nplt.figure(figsize=(10,4))\nplt.hist(r_means, bins=32, alpha=0.6, label='R')\nplt.hist(g_means, bins=32, alpha=0.6, label='G')\nplt.hist(b_means, bins=32, alpha=0.6, label='B')\nplt.legend()\nplt.title('Mean Color Distribution per Image')\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T02:34:40.240877Z","iopub.execute_input":"2025-10-18T02:34:40.242863Z","iopub.status.idle":"2025-10-18T02:34:55.097777Z","shell.execute_reply.started":"2025-10-18T02:34:40.242819Z","shell.execute_reply":"2025-10-18T02:34:55.096580Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n# **4. Conduct a small augmentation probe (e.g., crop, flip, color jitter) to determine which transformations are safe versus harmful for the dataset.**\n","metadata":{}},{"cell_type":"code","source":"import math\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras import layers, models\n\nPROBE_IMG_SIZE = (128, 128)\nBATCH = 32\nEPOCHS = 3\n\n# baseline generator with validation split\nbase_gen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\ntrain_gen_base = base_gen.flow_from_directory(TRAIN_DIR, target_size=PROBE_IMG_SIZE,\n                                              batch_size=BATCH, class_mode='categorical',\n                                              subset='training', shuffle=True, seed=123)\nval_gen_base = base_gen.flow_from_directory(TRAIN_DIR, target_size=PROBE_IMG_SIZE,\n                                            batch_size=BATCH, class_mode='categorical',\n                                            subset='validation', shuffle=False, seed=123)\n\nnum_classes = train_gen_base.num_classes\nprint(\"Number of classes:\", num_classes)\n\ndef make_small_model(input_shape=(128,128,3), num_classes=2):\n    m = models.Sequential([\n        layers.Conv2D(32, (3,3), activation='relu', input_shape=input_shape),\n        layers.MaxPool2D(2),\n        layers.Conv2D(64, (3,3), activation='relu'),\n        layers.MaxPool2D(2),\n        layers.Flatten(),\n        layers.Dense(64, activation='relu'),\n        layers.Dense(num_classes, activation='softmax')\n    ])\n    m.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n    return m\n\nprobe_configs = {\n    \"no_aug\": ImageDataGenerator(rescale=1./255, validation_split=0.2),\n    \"flip\": ImageDataGenerator(rescale=1./255, horizontal_flip=True, validation_split=0.2),\n    \"rotation_shift\": ImageDataGenerator(rescale=1./255, rotation_range=20,\n                                         width_shift_range=0.1, height_shift_range=0.1, validation_split=0.2),\n    \"color_jitter\": ImageDataGenerator(rescale=1./255, brightness_range=(0.6,1.4), validation_split=0.2),\n    \"heavy_crop\": ImageDataGenerator(rescale=1./255, zoom_range=(0.7,1.0), validation_split=0.2)\n}\n\nprobe_results = {}\n\nfor name, gen in probe_configs.items():\n    print(f\"\\n--- Running Probe: {name}\")\n    train_gen = gen.flow_from_directory(TRAIN_DIR, target_size=PROBE_IMG_SIZE,\n                                        batch_size=BATCH, class_mode='categorical',\n                                        subset='training', shuffle=True, seed=42)\n    val_gen = gen.flow_from_directory(TRAIN_DIR, target_size=PROBE_IMG_SIZE,\n                                      batch_size=BATCH, class_mode='categorical',\n                                      subset='validation', shuffle=False, seed=42)\n    model = make_small_model(input_shape=PROBE_IMG_SIZE+(3,), num_classes=num_classes)\n    steps = min(50, math.ceil(train_gen.samples / BATCH))\n    val_steps = min(20, math.ceil(val_gen.samples / BATCH))\n    history = model.fit(train_gen, epochs=EPOCHS, steps_per_epoch=steps,\n                        validation_data=val_gen, validation_steps=val_steps, verbose=1)\n    probe_results[name] = {\n        'train_acc': history.history['accuracy'][-1],\n        'val_acc': history.history['val_accuracy'][-1]\n    }\n\n# summarize probe results\nprobe_df = pd.DataFrame(probe_results).T\ndisplay(probe_df)\n\nbest = probe_df['val_acc'].idxmax()\nprint(f\"\\nBest probe config: {best} (val_acc = {probe_df.loc[best, 'val_acc']:.4f})\")\nprobe_df.to_csv(\"augmentation_probe_results.csv\")\nprint(\"Saved augmentation_probe_results.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T02:38:37.509135Z","iopub.execute_input":"2025-10-18T02:38:37.509560Z","iopub.status.idle":"2025-10-18T02:48:57.820170Z","shell.execute_reply.started":"2025-10-18T02:38:37.509529Z","shell.execute_reply":"2025-10-18T02:48:57.819274Z"}},"outputs":[],"execution_count":null}]}